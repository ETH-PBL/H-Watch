{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"tinyml_covid_cough_classifier.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"DPOQ8O4oce5i"},"source":["# TinyML for COVID-19 coughs detection"]},{"cell_type":"code","metadata":{"id":"J1ZeSmCnPRK2"},"source":["!pip -q install librosa==0.8.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWMoIL2UJmLz"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from time import time\n","import librosa, librosa.display\n","import matplotlib\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","# plt.switch_backend('agg')\n","\n","import itertools\n","import scipy\n","from scipy import signal\n","from scipy.fftpack import dct\n","from scipy.stats import zscore\n","\n","# from sklearn.preprocessing import LabelEncoder, scale, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","# from sklearn.decomposition import PCA\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, \\\n","    Flatten, BatchNormalization, ReLU, DepthwiseConv2D, SeparableConv2D, AveragePooling2D\n","# from tensorflow.keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D, UpSampling2D, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.utils import plot_model\n","\n","rand_seed = 10\n","np.random.seed(rand_seed)\n","tf.random.set_seed(rand_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5PYhD5l8JKI"},"source":["class MFCC:\n","\n","    def __init__(self, audio_filename, sample_rate=None, pre_emphasis_alpha=None,\n","                 frame_size=0.025, frame_stride=0.01, num_fft=512,\n","                 num_mel_filters=40, num_mfcc=12,\n","                 liftering=False, normalization=False):\n","        if sample_rate is not None:\n","            self.sample_rate = sample_rate\n","        else:\n","            self.sample_rate = 22050  # default sample rate of librosa\n","        \n","        self.audio, sr = librosa.load(audio_filename, self.sample_rate)\n","        assert sr == self.sample_rate\n","        \n","        self.alpha = pre_emphasis_alpha\n","        self.frame_size = frame_size\n","        self.frame_stride = frame_stride\n","        self.num_fft = num_fft\n","        self.num_mel_filters = num_mel_filters\n","        self.num_mfcc = num_mfcc\n","        self.liftering = liftering\n","        self.normalization = normalization\n","        \n","        # Keep the first 3.5 seconds\n","        # self.audio = self.audio[0:int(3.5 * self.sample_rate)]\n","    \n","    def emphasize(self):\n","        \"\"\"\n","        Apply a pre-emphasis filter on the signal to amplify the high frequencies.\n","        A pre-emphasis filter is useful in several ways:\n","        (1) balance the frequency spectrum since high frequencies usually have \n","            smaller magnitudes compared to lower frequencies,\n","        (2) avoid numerical problems during the Fourier transform operation and \n","        (3) may also improve the Signal-to-Noise Ratio (SNR).\n","\n","        The pre-emphasis filter can be applied to a signal x using the first order filter \n","        in the following equation:\n","        x(t) = x(t) - alpha * x(t-1)\n","        \"\"\"\n","        self.audio = np.append(self.audio[0], self.audio[1:] - pre_emphasis * self.audio[:-1])\n","    \n","    def get_frames():\n","        \"\"\"\n","        Split the signal into short-time frames.\n","        The rationale behind this step is that frequencies in a signal change over time,\n","        so in most cases it doesn’t make sense to do the Fourier transform across the entire signal \n","        in that we would lose the frequency contours of the signal over time.\n","        To avoid that, we can safely assume that frequencies in a signal are stationary \n","        over a very short period of time.\n","        Therefore, by doing a Fourier transform over this short-time frame, we can obtain \n","        a good approximation of the frequency contours of the signal by concatenating adjacent frames.\n","\n","        Typical frame sizes in speech processing range from 20 ms to 40 ms with \n","        50% (+/-10%) overlap between consecutive frames.\n","        Popular settings are 25 ms for the frame size, frame_size = 0.025 and \n","        a 10 ms stride (15 ms overlap), frame_stride = 0.01.\n","        \"\"\"\n","        # Convert from seconds to samples\n","        frame_length = self.frame_size * self.sample_rate\n","        frame_step = self.frame_stride * self.sample_rate\n","        \n","        signal_length = len(self.audio)\n","        frame_length = int(round(frame_length))\n","        frame_step = int(round(frame_step))\n","        # Make sure that we have at least 1 frame\n","        num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n","\n","        pad_signal_length = num_frames * frame_step + frame_length\n","        zero_padding = np.zeros((pad_signal_length - signal_length))\n","        # Pad Signal to make sure that all frames have equal number of samples \n","        # without truncating any samples from the original signal\n","        pad_signal = np.append(self.audio, zero_padding)\n","\n","        indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n","        frames = pad_signal[indices.astype(np.int32, copy=False)]\n","    \n","    def mel_fileterbanks(self, pow_frames, normalization=False):\n","        low_freq_mel = 0\n","        high_freq_mel = (2595 * np.log10(1 + (self.sample_rate / 2) / 700))  # Convert Hz to Mel\n","        # Equally spaced points in Mel scale\n","        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.num_mel_filters + 2)\n","        hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n","        bin = np.floor((self.num_fft + 1) * hz_points / self.sample_rate)\n","\n","        fbank = np.zeros((self.num_mel_filters, int(np.floor(self.num_fft / 2 + 1))))\n","        for m in range(1, nfilt + 1):\n","            f_m_minus = int(bin[m - 1])   # left\n","            f_m = int(bin[m])             # center\n","            f_m_plus = int(bin[m + 1])    # right\n","\n","        for k in range(f_m_minus, f_m):\n","            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n","        for k in range(f_m, f_m_plus):\n","            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n","\n","        filter_banks = np.dot(pow_frames, fbank.T)\n","        filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks) # Numerical Stability\n","        filter_banks = 20 * np.log10(filter_banks)  # dB\n","\n","        if self.normalization:\n","            filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)\n","\n","        return filter_banks\n","    \n","    def mfcc(self):\n","        # 1) Split the audio signal into short-time frames:\n","        frames = self.get_frames()\n","        \n","        # 2) Apply a window function such as the Hamming window to each frame.\n","        # A Hamming window has the following form:\n","        # w[n] = 0.54 - 0.46 * cos( (2* pi * n) / (N - 1) )\n","        frames *= numpy.hamming(frame_length)\n","        # Explicit Implementation:\n","        # frames *= 0.54 - 0.46 * numpy.cos((2 * numpy.pi * n) / (frame_length - 1)) \n","\n","        # 3) Compute the power spectrum\n","        # To achieve this goal we first need to compute the frequency spectrum, \n","        # through an N-point FFT on each frame, which is also called \n","        # Short-Time Fourier-Transform (STFT), where N is typically 256 or 512,\n","        # and then compute the power spectrum (periodogram) using the following equation:\n","        # P = (|FFT(x_i)|^2) / N, where x_i is the i-th frame of the audio signal x\n","        mag_frames = numpy.absolute(numpy.fft.rfft(frames, self.num_fft))  # Magnitude of the FFT\n","        pow_frames = ((1.0 / self.num_fft) * ((mag_frames) ** 2))  # Power Spectrum\n","\n","        # 4) Process the frames power spectrum using the MEL Filterbanks\n","        mel_filtbanks_coeff = self.mel_fileterbanks(pow_frames)\n","\n","        # 5) Apply Discrete Cosine Transform (DCT)\n","        # Usually filter bank coefficients computed in the previous step are highly correlated,\n","        # which could be problematic in some machine learning algorithms.\n","        # Therefore, we can apply Discrete Cosine Transform (DCT) to decorrelate \n","        # the filter bank coefficients and yield a compressed representation of the filter banks.\n","        # Typically, for Automatic Speech Recognition (ASR), the resulting cepstral coefficients 2-13\n","        # are retained and the rest are discarded; num_mfcc = 12.\n","        # The reasons for discarding the other coefficients is that they represent fast changes \n","        # in the filter bank coefficients and these fine details don’t contribute to ASR.\n","        mfcc = dct(mel_filtbanks_coeff, type=2, axis=1, norm='ortho')[:, 1 : (self.num_mfcc + 1)] # Keep 2-13\n","\n","        # 6) Optionally apply sinusoidal liftering to the MFCCs:\n","        # It can be used to de-emphasize higher MFCCs which has been claimed to improve \n","        # speech recognition in noisy signals.\n","        if self.liftering:\n","            (nframes, ncoeff) = mfcc.shape\n","            n = np.arange(ncoeff)\n","            lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n","            mfcc *= lif\n","        \n","        # 7) Optionally apply Mean normalization:\n","        # Mean normalization can be used to balance the spectrum and improve the Signal-to-Noise (SNR),\n","        # by subtract the mean of each coefficient from all frames.\n","        if self.normalization:\n","            mfcc = zscore(mfcc, axis=0)\n","            # Equivalent to:\n","            # mean = np.mean(mfcc, axis=0) # + 1e-8\n","            # std_dev = np.std(mfcc, axis=0)\n","            # Two main ways to handle zero std_dev (when all samples are equal)\n","            # 1) add a small eps:\n","            # std_dev += 1e-8\n","            # 2) replace zero std_dev with 1\n","            # std_dev = [1. if std_i == 0. else std_i for std_i in std_dev]\n","            # mfcc = (mfcc - mean) / std_dev\n","        \n","        return mfcc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wu2V97Nce5i"},"source":["class AudioClassifier():\n","    def __init__(self, dataset_path=\"audio\", labels_file='cough_dataset.csv',\n","                 model_name=\"tinyconv1\", num_covid_audio=17, num_other_audio=30,\n","                 sample_rate=None, audio_max_samples=None, augment=True,\n","                 frame_length=None, hop_length=None, num_mfcc_frames=None, \n","                 num_mfcc=20, num_fft=2048, num_mels=128, normalize=False, \n","                 batch_size=4, lr=0.001, epochs=100, plot=False):\n","        self.target_names = ['not_covid', 'covid']\n","        self.dataset_path = dataset_path\n","        self.labels_file = labels_file\n","        self.num_covid = num_covid_audio\n","        self.num_notcovid = num_other_audio\n","        self.model_name = model_name\n","        \n","        if sample_rate is not None and sample_rate > 0:\n","            self.sample_rate = sample_rate\n","        else:\n","            self.sample_rate = 22050  # default sample rate of librosa: 22.05 kHz\n","        \n","        if audio_max_samples is not None and audio_max_samples < 0:\n","            # raise ValueError(\"Invalid value for audio_max_samples: {}\".format(audio_max_samples))\n","            audio_max_samples = None\n","        self.audio_max_samples = audio_max_samples\n","\n","        if augment not in [True, False]:\n","            raise ValueError(\"Invalid value for augment: {}\".format(augment))\n","        self.augment = augment\n","        \n","        if frame_length is None and frame_length < 0:\n","            self.frame_length = num_fft   # default value for librosa\n","        else:\n","            self.frame_length = frame_length\n","        \n","        if hop_length is None and hop_length < 0:\n","            self.hop_length = num_fft // 4   # default value for librosa\n","        else:\n","            self.hop_length = hop_length\n","        \n","        if num_mfcc_frames is not None and num_mfcc_frames < 0:\n","            # raise ValueError(\"Invalid value for num_mfcc_frames: {}\".format(num_mfcc_frames))\n","            num_mfcc_frames = None\n","        self.num_mfcc_frames = num_mfcc_frames\n","        \n","        self._check_pos_param(num_mfcc)\n","        self.num_mfcc = num_mfcc\n","        \n","        self._check_pos_param(num_fft)\n","        self.num_fft = num_fft\n","        \n","        self._check_pos_param(num_mels)\n","        self.num_mels = num_mels\n","        \n","        if normalize not in [True, False]:\n","            raise ValueError(\"Invalid value for normalize: {}\".format(normalize))\n","        self.normalize = normalize\n","\n","        self._check_pos_param(batch_size)\n","        self.batch_size = batch_size\n","        self._check_pos_param(lr)\n","        self.lr = lr\n","        self._check_pos_param(epochs)\n","        self.epochs = epochs\n","        if plot not in [True, False]:\n","            raise ValueError(\"Invalid value for plot: {}\".format(plot))\n","        self.plot = plot\n","\n","        \"\"\"\n","        MFCC settings to try:\n","        1) https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\n","           num_fft = 512, num_mels = 40, num_mfcc = 13 (2-12)\n","        2) http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/:\n","           num_fft = 512, num_mels = 26, num_mfcc = 13 (2-12)\n","        3) Default settings of librosa:\n","           num_mfcc=20, num_fft=2048, num_mels=128\n","        \"\"\"\n","\n","        # if the dataset file does not exists, download it\n","        if self.dataset_path is None or not os.path.isdir(self.dataset_path) or \\\n","                self.labels_file is None or not os.path.isfile(self.labels_file):\n","            print(\"Invalid dataset path. Dowloading the dataset ...\")\n","            os.system(\"wget -O ./data.zip https://www.dropbox.com/sh/mjyrspykfx116lo/AADb3-7_SpUpF90LcPkbRV3Ga?dl=1\")\n","            os.system(\"mkdir -p dataset && unzip data.zip -d dataset\")\n","\n","            self.dataset_path = os.path.join(os.getcwd(), \"dataset/audio/\")\n","            self.labels_file = os.path.join(os.getcwd(), \"dataset/cough_dataset.csv\")\n","        \n","        os.system(\"mkdir -p plots\")\n","        os.system(\"mkdir -p mfcc\")\n","    \n","    def _check_pos_param(self, param):\n","        if param is None or param < 0:\n","            raise ValueError(\"Invalid value for {}: {}\".format(param.__name__, param))\n","    \n","    def _save_augmented(self, aug_audio, audio_name, aug_type):\n","        augmented_name = audio_name[:-4] + \"_\" + aug_type + \".wav\"\n","        librosa.output.write_wav(augmented_name, aug_audio, self.sample_rate)\n","\n","        return augmented_name\n","    \n","    def augment_audio(self, audio_names, audio_labels):\n","        augmented_audios = np.array([])\n","        augmented_labels = np.array([])\n","\n","        for name, label in zip(audio_names, audio_labels):\n","            file_path = os.path.join(self.dataset_path, name)\n","            audio, sr = librosa.load(file_path, self.sample_rate)\n","            augmented_audios.append(name)\n","            # repeat the same label for all the augmented samples,\n","            # including also the label of the original sample (5 + 1 = 6 labels)\n","            augmented_labels = np.append(augmented_labels, [label] * 6)\n","\n","            # 1) Noise Injection:\n","            # Add some random white noise using a noise factor of 0.005 to limit\n","            # the noise amplitude:\n","            noise = np.random.randn(len(audio))\n","            augmented_audio = audio + 0.005 * noise\n","            # Cast back to same data type\n","            augmented_audio = augmented_audio.astype(type(audio[0]))\n","            # augmented_name = name[:-4] + \"_noise.wav\"\n","            # librosa.output.write_wav(augmented_name, augmented_audio, sr)\n","            aug_name = self._save_augmented(augmented_audio, name, \"noise\")\n","            augmented_audios = np.append(augmented_audios, aug_name)\n","\n","            # 2) Random shift:\n","            # Randomly shift audio to left/right with a random second. If shifting audio to left \n","            # (fast forward) with x seconds, first x seconds will mark as 0 (i.e. silence).\n","            # If shifting audio to right (back forward) with x seconds, last x seconds will mark as 0.\n","            max_shift = 3  # max shift in seconds\n","            shift = np.random.randint(sr * shift_max)  # by default, use fast-forward shift\n","            # Uncommment the following line for back forward shift:\n","            # shift = -shift\n","            augmented_audio = np.roll(audio, shift)\n","            # Set to silence for heading/ tailing\n","            if shift > 0:\n","                augmented_audio[:shift] = 0\n","            else:\n","                augmented_audio[shift:] = 0\n","            aug_name = self._save_augmented(augmented_audio, name, \"shift\")\n","            augmented_audios = np.append(augmented_audios, aug_name)\n","\n","            # 3) Change pitch randomly:\n","            # how many (fractional) steps to shift the pitch\n","            # A step is equal to a semitone if bins_per_octave is set to 12 (default)\n","            pitch_factor = 3\n","            augmented_audio = librosa.effects.pitch_shift(audio, sr, pitch_factor)\n","            aug_name = self._save_augmented(augmented_audio, name, \"pitch\")\n","            augmented_audios = np.append(augmented_audios, aug_name)\n","\n","            # 4) Change speed randomly:\n","            # Time-stretch an audio series by a fixed rate\n","            # If speed_factor > 1, then the signal is sped up.\n","            # Otherwise, if speed_factor < 1, then the signal is slowed down.\n","            speed_factor = 1.5\n","            augmented_audio = librosa.effects.time_stretch(audio, speed_factor)\n","            aug_name = self._save_augmented(augmented_audio, name, \"speed\")\n","            augmented_audios = np.append(augmented_audios, aug_name)\n","\n","            # 5) Normalize (min-max normalization)\n","            lower = np.min(np.abs(audio))\n","            augmented_audio = (data - lower) / (np.max(np.abs(data)) - lower)\n","            aug_name = self._save_augmented(augmented_audio, name, \"speed\")\n","            augmented_audios = np.append(augmented_audios, aug_name)\n","        \n","        # shuffle the list of augmented audio names and labels\n","        rand_idx = np.arange(len(augmented_audios))\n","        np.random.shuffle(rand_idx)\n","        augmented_audios = augmented_audios[rand_idx]\n","        augmented_labels = augmented_labels[rand_idx]\n","\n","        return augmented_audios, augmented_labels\n","\n","\n","    def get_mfcc(self, file_name):\n","        try:\n","            \"\"\"\n","            Load and preprocess the audio\n","            \"\"\"\n","            audio, sr = librosa.load(file_name, self.sample_rate)\n","            assert sr == self.sample_rate\n","            length_s = audio.shape[0] / float(sr)\n","\n","            # print(\"Loaded audio file: {}, num samples, length: {} s\".format(file_name, len(audio), length_s))\n","            if self.plot:\n","                self.plot_audio(audio, file_name, length_s)\n","\n","            # Remove vocals using foreground separation:\n","            # https://librosa.github.io/librosa_gallery/auto_examples/plot_vocal_separation.html\n","            # y_no_vocal = self.vocal_removal(audio, sample_rate)\n","\n","            # Remove noise using median smoothing\n","            # y = self.reduce_noise_median(audio, sample_rate)\n","\n","            # Only use audio above the human vocal range (85-255 Hz)\n","            # fmin = 260\n","            # fmax = 10000\n","\n","            # Audio slices\n","            y = audio\n","            \n","            # Take the first self.audio_max_samples samples of the audio:\n","            y = y[:self.audio_max_samples]\n","\n","            # Extract MFCC features\n","            \"\"\"\n","            max_pad_length = 431\n","            n_mfcc = 120\n","            n_fft = 4096\n","            hop_length = 512\n","            n_mels = 512\n","            \n","            # mfccs = librosa.feature.mfcc(y=y, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)\n","            mfccs = librosa.feature.mfcc(y=y, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n","            pad_width = max_pad_length-mfccs.shape[1]\n","            mfccs = np.pad(mfccs, pad_width=((0,0),(0,pad_width)), mode='constant')\n","            # print(mfccs.shape)\n","            # mfccs_scaled = np.mean(mfccs.T, axis=0)\n","            \"\"\"\n","            mfccs = librosa.feature.mfcc(y=y, sr=self.sample_rate,\n","                                         n_mfcc=self.num_mfcc, \n","                                         n_fft=self.num_fft,\n","                                         n_mels=self.num_mels,\n","                                         win_length=self.frame_length,\n","                                         hop_length=self.hop_length\n","                                         )\n","            if self.plot:\n","                self.plot_mfcc(file_name, mfccs, sr, time=length_s)\n","\n","        except Exception as e:\n","            print(\"Error encountered while processing file: {}\".format(e))\n","            return None, None, None \n","\n","        return mfccs, self.sample_rate, length_s\n","    \n","    def plot_audio(self, audio, name, time_length):\n","        fig = plt.figure(figsize=(10, 4))\n","        plt.title(\"Audio sample {}\".format(name))\n","        plt.ylabel(\"Amplitude\")\n","        plt.plot(np.linspace(0, time_length, len(audio)), audio)\n","        plt.show()\n","    \n","    def load_data(self, data_names, data_labels):\n","        mfcc_data = None\n","        labels = []\n","\n","        for name, label in zip(data_names, data_labels):\n","            file_path = os.path.join(self.dataset_path, name)\n","            mfcc, sample_rate, time_len = self.get_mfcc(file_path)\n","            if mfcc is not None:\n","                label_str = \"covid\" if label == 1. else \"not_covid\"\n","                print(\"Loaded audio: {}, Label: {}\".format(name, label_str))\n","                print(\"MFCC initial shape: {}\".format(mfcc.shape))\n","                n_mfcc, n_frames = mfcc.shape\n","\n","                if self.num_mfcc_frames is not None:\n","                    if n_frames >= self.num_mfcc_frames:\n","                        # take only the needed MFCC frames\n","                        mfcc = mfcc[:, :self.num_mfcc_frames]\n","                    else:\n","                        # padd with zero\n","                        pad_width = self.num_mfcc_frames - n_frames\n","                        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","\n","                if self.normalize:\n","                    # mfcc = mfcc.T  # transpose mfcc -> shape: (n_frames, n_mfcc)\n","                    # Normalize data:\n","                    # NOTE: mfcc array is a 2D matrix of shape: (n_mfcc, n_frames).\n","                    # Rows represent features (MFCC), while columns represent samples (frames).\n","                    # We need to apply z-score normalization feature-wise, so considering all\n","                    # the frames related to a specific MFCC feature.\n","                    # So, we need to normalize along axis=1 (horizontally across columns / frames). \n","                    # mfcc = zscore(mfcc, axis=1)\n","                    mean = np.mean(mfcc, axis=1).reshape(-1, 1) # + 1e-8\n","                    std_dev = np.std(mfcc, axis=1)\n","                    std_dev = np.array([1. if std_i == 0. else std_i for std_i in std_dev]).reshape(-1, 1)\n","                    mfcc = (mfcc - mean) / std_dev\n","                    # mfcc = mfcc.T\n","                \n","                if self.plot:\n","                    self.plot_mfcc(name + \" Processed\", mfcc, sample_rate, time=time_len)\n","\n","                mfcc = mfcc.reshape(1, mfcc.shape[0], mfcc.shape[1], 1)\n","\n","                print(\"MFCC new shape: {}\".format(mfcc.shape))\n","\n","                if mfcc_data is None:\n","                    mfcc_data = mfcc\n","                else:\n","                    mfcc_data = np.append(mfcc_data, mfcc, axis=0)\n","                # labels.append(1. if label is 'covid' else 0.)\n","                labels.append(label)\n","\n","            else:\n","                print(\"Cannot load {}\".format(file_path))\n","        \n","        # stop = input(\"Stop\")\n","        return mfcc_data, np.array(labels)\n","\n","    def run(self):\n","        colnames = ['name', 'label']\n","        audio_names = []\n","        audio_labels = []\n","\n","        audio_samples = pd.read_csv(self.labels_file, names=colnames, header=None)\n","        \n","        covid_samples = audio_samples.loc[audio_samples['label'] == \"covid\"]\n","        other_samples = audio_samples.loc[audio_samples['label'] == \"not_covid\"]\n","\n","        print(\"covid_samples: {}\".format(covid_samples))\n","        print(\"other_samples: {}\".format(other_samples))\n","\n","        # shuffle df and take the needed samples\n","        covid_samples = covid_samples.sample(frac=1, random_state=rand_seed).reset_index(drop=True)[:self.num_covid]\n","        other_samples = other_samples.sample(frac=1, random_state=rand_seed).reset_index(drop=True)[:self.num_notcovid]\n","\n","        print(\"covid_samples shuffled: {}\".format(covid_samples))\n","        print(\"other_samples shuffled: {}\".format(other_samples))\n","\n","        audio_names.extend(covid_samples.name.tolist())\n","        audio_names.extend(other_samples.name.tolist())\n","\n","        audio_labels.extend(covid_samples.label.tolist())\n","        audio_labels.extend(other_samples.label.tolist())\n","\n","        print(\"audio_names: {}\".format(audio_names))\n","        print(\"audio_labels: {}\".format(audio_labels))\n","\n","        audio_labels = [1. if l == \"covid\" else 0. for l in audio_labels]\n","\n","        self.num_classes = len(np.unique(audio_labels))\n","\n","        # AUGMENTATION NOTES:\n","        # 1st Method:\n","        # By default we use 30 non-covid samples and 17 covid samples, so in total 47 samples.\n","        # To solve the issue of the low number of samples, we adopt the following approach:\n","        # 1) Split the initial dataset into train (60 % => 28 samples) and test (40 % => 19)\n","        # 2) Further split the second set (test) into validation (40 % of 19 => 8) \n","        #    and test (60 % of 19 => 11) sets\n","        # 3) Apply augmentation techiniques to train samples to increase its size (x4)\n","        #\n","        # 2nd Method:\n","        # Apply data augmentation to the original dataset to increase its size and then the split \n","        # the augmented dataset into training, validation and test.\n","        # The main disadvantage is that usually validation and test sets should not be augmented,\n","        # but in this case it should be fine since the initial size of the sataset is too low to\n","        # obtain good performance and above all to avoid overfitting.\n","        # In this approach we adopt a train-validation-test split equal to 70-15-15\n","\n","        # Use 2nd method for data augmentation:\n","        if self.augment:\n","            audio_names, audio_labels = self.augment_audio(audio_names, audio_labels)\n","        \n","        train_names, test_names, train_labels, test_labels = train_test_split(\n","            audio_names, audio_labels, test_size=0.3, random_state=rand_seed, stratify=audio_labels)\n","        \n","        test_names, val_names, test_labels, val_labels = train_test_split(\n","            test_names, test_labels, test_size=0.5, random_state=rand_seed, stratify=test_labels)\n","        \n","        print(\"train_names: {}, num_train: {}\".format(train_names, len(train_names)))\n","        print(\"train_labels: {}\".format(train_labels))\n","        assert len(train_names) == len(train_labels)\n","        print(\"val_names: {}, num_val: {}\".format(val_names, len(val_names)))\n","        print(\"val_labels: {}\".format(val_labels))\n","        assert len(val_names) == len(val_labels)\n","        print(\"test_names: {}, num_test: {}\".format(test_names, len(test_names)))\n","        print(\"test_labels: {}\".format(test_labels))\n","        assert len(test_names) == len(test_labels)\n","        \n","        X_train, y_train = self.load_data(train_names, train_labels)\n","        X_test, y_test = self.load_data(test_names, test_labels)\n","        X_val, y_val = self.load_data(val_names, val_labels)\n","\n","        print(\"X_train shape: {}\".format(X_train.shape))\n","        print(\"y_train: {}, shape: {}\".format(y_train, y_train.shape))\n","        print(\"X_val shape: {}\".format(X_val.shape))\n","        print(\"y_val: {}, shape: {}\".format(y_val, y_val.shape))\n","        print(\"X_test shape: {}\".format(X_test.shape))\n","        print(\"y_test: {}, shape: {}\".format(y_test, y_test.shape))\n","\n","        input_shape = X_train[0].shape\n","        if self.model_name == \"tinyconv1\":\n","            model = self.build_tinyconv1_model(input_shape)\n","        elif self.model_name == \"tinyconv2\":\n","            model = self.build_tinyconv2_model(input_shape)\n","        elif self.model_name == \"tinyconv3\":\n","            model = self.build_tinyconv3_model(input_shape)\n","        elif self.model_name == \"tinyconv4\":\n","            model = self.build_tinyconv4_model(input_shape)\n","\n","        opt = optimizers.Adam(lr=self.lr)\n","        model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n","        model.summary()\n","        plot_model(model, to_file='plots/model.png', show_shapes=True, show_layer_names=True)\n","\n","        # stop = input(\"Stop\")\n","\n","        # Calculate pre-training accuracy\n","        # score = model.evaluate(X_test, y_test, verbose=1)\n","        # print(\"Pre-training accuracy (on test_set): {}\".format(100 * score[1]))\n","\n","        # Train the model\n","        checkpoint = ModelCheckpoint(filepath='saved_models/keras_model.h5',\n","                                     monitor='val_accuracy',\n","                                     verbose=1,\n","                                     save_best_only=True)\n","        # es_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n","        # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.001, patience=7, verbose=1, mode='auto', min_delta=0.001, cooldown=1, min_lr=0)\n","        \n","        start = time() # datetime.now()\n","        history = model.fit(X_train, y_train, batch_size=self.batch_size,\n","                            epochs=self.epochs,\n","                            validation_data=(X_val, y_val),\n","                            shuffle=False,\n","                            callbacks=[checkpoint],\n","                            verbose=2)\n","        # history = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), shuffle=True, callbacks=[checkpointer], verbose=1)\n","        duration = time() - start  # datetime.now() - start\n","        print(\"Training completed in {:.2f} s\".format(duration))\n","\n","        # Evaluating the model on the training and testing set\n","        score = model.evaluate(X_train, y_train, verbose=0)\n","        print(\"Training accuracy: {:.3f}\".format(score[1] * 100))\n","\n","        score = model.evaluate(X_test, y_test, verbose=0)\n","        print(\"Testing accuracy: {:.3f}\".format(score[1] * 100))\n","\n","        # Plots and reports\n","        self.plot_history(history)\n","\n","        y_pred = np.argmax(model.predict(X_test), axis=1)\n","        print(\"Target labels (test set): {}\".format(y_test))\n","        print(\"Predicted labels (test set): {}\".format(y_pred))\n","        cm = confusion_matrix(y_test, y_pred)\n","        self.plot_confusion_matrix(cm, self.target_names)\n","        self.plot_classification_report(y_test, y_pred)\n","\n","    def build_tinyconv1_model(self, input_shape):\n","        model = Sequential()\n","        \n","        model.add(Input(shape=input_shape, name=\"input\"))\n","        model.add(Conv2D(filters=8, kernel_size=(10, 8), strides=2, padding=\"same\",\n","                         activation='relu', name=\"conv\"))\n","        # model.add(BatchNormalization(name=\"bn\"))\n","        model.add(Dropout(0.5, name=\"dropout\"))\n","        model.add(Flatten(name=\"flatten\"))\n","        model.add(Dense(self.num_classes, activation='softmax', name=\"dense\"))\n","\n","        return model\n","    \n","    def build_tinyconv2_model(self, input_shape):\n","        model = Sequential()\n","        \n","        model.add(Input(shape=input_shape, name=\"input\"))\n","        \n","        model.add(Conv2D(filters=32, kernel_size=(20, 8), strides=1, padding=\"same\",\n","                         activation='relu', name=\"conv1\"))\n","        # model.add(BatchNormalization(name=\"bn\"))\n","        model.add(Dropout(0.25, name=\"drop1\"))\n","        model.add(MaxPooling2D(pool_size=(2, 2), name=\"pool1\"))\n","\n","        model.add(Conv2D(filters=64, kernel_size=(10, 4), strides=1, padding=\"same\",\n","                         activation='relu', name=\"conv2\"))\n","        # model.add(BatchNormalization(name=\"bn\"))\n","        model.add(Dropout(0.5, name=\"drop2\"))\n","        model.add(MaxPooling2D(pool_size=(2, 2), name=\"pool2\"))\n","\n","        model.add(Flatten(name=\"flatten\"))\n","\n","        model.add(Dense(self.num_classes, activation='softmax', name=\"dense\"))\n","\n","        return model\n","    \n","    def build_tinyconv3_model(self, input_shape):\n","        model = Sequential()\n","        \n","        model.add(Input(shape=input_shape, name=\"input\"))\n","        \n","        model.add(Conv2D(filters=32, kernel_size=(3, 3),\n","                         activation='relu', name=\"conv1\"))\n","        model.add(Conv2D(filters=64, kernel_size=(3, 3),\n","                         activation='relu', name=\"conv2\"))\n","        model.add(MaxPooling2D(pool_size=(2, 2), name=\"pool1\"))\n","        model.add(Dropout(0.25, name=\"drop1\"))\n","        model.add(Flatten(name=\"flatten\"))\n","        model.add(Dense(units=128, activation='relu', name=\"dense1\"))\n","        model.add(Dropout(0.5, name=\"drop2\"))\n","        model.add(Dense(self.num_classes, activation='softmax', name=\"dense2\"))\n","\n","        return model\n","    \n","    def build_tinyconv4_model(self, input_shape):\n","        model = Model()\n","        \n","        input = Input(shape=input_shape)\n","        \n","        conv1 = Conv2D(filters=32, kernel_size=(3, 3), strides=2, padding=\"same\", name=\"conv1\")(input)\n","        bn1 = BatchNormalization(name=\"bn1\")(conv1)\n","        # relu1 = ReLU(name=\"relu1\")(bn1)\n","        relu1 = ReLU(max_value=6, name=\"relu1\")(bn1)\n","        \n","        dw2 = DepthwiseConv2D(kernel_size=(3, 3), strides=2, padding=\"same\", name=\"dw2\")(relu1)\n","        bn2 = BatchNormalization(name=\"bn2\")(dw2)\n","        # relu2 = ReLU(name=\"relu2\")(bn2)\n","        relu2 = ReLU(max_value=6, name=\"relu2\")(bn2)\n","\n","        pw3 = Conv2D(filters=64, kernel_size=(1, 1), strides=1, padding=\"same\", name=\"pw3\")(relu2)\n","        bn3 = BatchNormalization(name=\"bn3\")(pw3)\n","        # relu3 = ReLU(name=\"relu3\")(bn3)\n","        relu3 = ReLU(max_value=6, name=\"relu3\")(bn3)\n","\n","        pool4 = AveragePooling2D(pool_size=(3, 3))(relu3)\n","        flatten = Flatten(name=\"flatten\")(pool4)\n","        dense5 = Dense(self.num_classes, activation='softmax', name=\"dense5\")(flatten)\n","\n","        model = Model(inputs=input, outputs=dense5)\n","\n","        return model\n","\n","    def plot_history(self, history):\n","        # Plot training & validation accuracy\n","        plt.plot(history.history['accuracy'])\n","        plt.plot(history.history['val_accuracy'])\n","        plt.title('Model accuracy')\n","        plt.ylabel('Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.legend(['Train', 'Validation'], loc='upper left')\n","        # plt.show()\n","        plt.savefig('plots/accuracy.png')\n","        plt.clf()\n","\n","        # Plot training & validation loss values\n","        plt.plot(history.history['loss'])\n","        plt.plot(history.history['val_loss'])\n","        plt.title('Model loss')\n","        plt.ylabel('Loss')\n","        plt.xlabel('Epoch')\n","        plt.legend(['Train', 'Validation'], loc='upper left')\n","        # plt.show()\n","        plt.savefig('plots/loss.png')\n","        plt.close()\n","\n","    def plot_classification_report(self, x_test, y_test):\n","        # Print\n","        print(classification_report(x_test, y_test, target_names=self.target_names))\n","        # Save data\n","        clsf_report = pd.DataFrame(classification_report(y_true = x_test, y_pred = y_test, output_dict=True, target_names=self.target_names)).transpose()\n","        clsf_report.to_csv('plots/classification_report.csv', index= True)\n","\n","    def plot_confusion_matrix(self, cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n","        matplotlib.rcParams.update({'font.size': 22})\n","        accuracy = np.trace(cm) / float(np.sum(cm))\n","        misclass = 1 - accuracy\n","\n","        if cmap is None:\n","            cmap = plt.get_cmap('Blues')\n","\n","        plt.figure(figsize=(14, 12))\n","        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","        plt.title(title)\n","        plt.colorbar()\n","\n","        if target_names is not None:\n","            tick_marks = np.arange(len(target_names))\n","            plt.xticks(tick_marks, target_names, rotation=45)\n","            plt.yticks(tick_marks, target_names)\n","\n","        if normalize:\n","            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","            if normalize:\n","                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                         horizontalalignment=\"center\",\n","                         color=\"white\" if cm[i, j] > thresh else \"black\")\n","            else:\n","                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                         horizontalalignment=\"center\",\n","                         color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","        plt.tight_layout()\n","        plt.ylabel('True label')\n","        plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","        plt.savefig('plots/confusion_matrix.png', bbox_inches = \"tight\")\n","        plt.close()\n","\n","    def plot_mfcc(self, filename, mfcc, sr, time=None):\n","        \n","        # plt.figure(figsize=(14, 8))\n","        # S_dB = librosa.power_to_db(mfcc, ref=np.max)\n","        # librosa.display.specshow(S_dB, y_axis='mel', x_axis='time')\n","        #\n","        # librosa.display.specshow(librosa.amplitude_to_db(mfcc, ref=np.max), y_axis='mel', x_axis='time', sr=sr)\n","        # plt.colorbar(format='%+2.0f dB')\n","        # plt.title(filename)\n","        # plt.ylabel('MFCC Coefficients', fontsize=12)\n","        # plt.xlabel('Time (s)', fontsize=12)\n","        # plt.tight_layout()\n","\n","        plt.figure(figsize=(10, 6))\n","        # plt.imshow(mfcc, cmap=cm.jet, aspect=0.3, extent=[0, time, 0, mfcc.shape[0]])\n","        sns.heatmap(mfcc)\n","        plt.ylabel('MFCC Coefficients', fontsize=12)\n","        plt.xlabel('# Frames', fontsize=12)\n","        plt.title(filename)\n","        \n","        # plt.savefig('mfcc/'+ filename + '.png', bbox_inches=None, pad_inches=0)\n","        plt.show()\n","\n","    def reduce_noise_median(self, y, sr):\n","        \"\"\"\n","            NOISE REDUCTION USING MEDIAN:\n","            receives an audio matrix,\n","            returns the matrix after gain reduction on noise\n","            https://github.com/dodiku/noise_reduction/blob/master/noise.py\n","        \"\"\"\n","        y = sp.signal.medfilt(y, 3)\n","        return y\n","\n","    def vocal_removal(self, y, sr):\n","        \"\"\"\n","        https://librosa.github.io/librosa_gallery/auto_examples/plot_vocal_separation.html\n","        \"\"\"\n","        idx = slice(*librosa.time_to_frames([0, 10], sr=sr))\n","        S_full, phase = librosa.magphase(librosa.stft(y))\n","        S_filter = librosa.decompose.nn_filter(S_full,\n","                                       aggregate=np.median,\n","                                       metric='cosine',\n","                                       width=int(librosa.time_to_frames(2, sr=sr)))\n","        S_filter = np.minimum(S_full, S_filter)\n","        margin_i, margin_v = 2, 10\n","        power = 2\n","        mask_i = librosa.util.softmask(S_filter,\n","                                       margin_i * (S_full - S_filter),\n","                                       power=power)\n","\n","        mask_v = librosa.util.softmask(S_full - S_filter,\n","                                       margin_v * S_filter,\n","                                       power=power)\n","\n","        S_foreground = mask_v * S_full\n","        S_background = mask_i * S_full\n","\n","        # Convert back to audio\n","        audio_minus_vocals = librosa.core.istft(S_background[:, idx])\n","\n","        return audio_minus_vocals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hd0IsdCZSrwB"},"source":["audioClassifier = AudioClassifier(dataset_path=\"dataset/audio/\",\n","                                  labels_file=\"dataset/cough_dataset.csv\",\n","                                  model_name=\"tinyconv1\",\n","                                  sample_rate=16000, frame_length=1024, hop_length=512,\n","                                  num_mfcc_frames=32, num_mfcc=12, num_fft=1024, num_mels=40, \n","                                  normalize=False, batch_size=4, lr=0.001, epochs=50, plot=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRqhn8EEVE5G","executionInfo":{"status":"ok","timestamp":1608834647000,"user_tz":-60,"elapsed":19064,"user":{"displayName":"Biagio Montaruli","photoUrl":"","userId":"02035397697813198636"}},"outputId":"9c58f4f0-e7d1-44e0-8367-37575d342731"},"source":["audioClassifier.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["covid_samples:                                               name  label\n","0   cough-shallow-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav  covid\n","1                      pos-0421-084-cough-m-50.wav  covid\n","2     cough-heavy-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav  covid\n","3     cough-heavy-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav  covid\n","4     cough-heavy-hte8VptUoGVFEqvHpbh5brgfcNP2.wav  covid\n","5   cough-shallow-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav  covid\n","6                      pos-0421-092-cough-m-53.wav  covid\n","7                      pos-0422-096-cough-m-31.wav  covid\n","8                      pos-0421-094-cough-m-51.wav  covid\n","9   cough-shallow-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav  covid\n","10                     pos-0421-086-cough-m-65.wav  covid\n","11                     pos-0421-087-cough-f-40.wav  covid\n","12    cough-heavy-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav  covid\n","13  cough-shallow-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav  covid\n","14    cough-heavy-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav  covid\n","15  cough-shallow-hte8VptUoGVFEqvHpbh5brgfcNP2.wav  covid\n","16                     pos-0421-093-cough-f-24.wav  covid\n","other_samples:                                   name      label\n","17   U362CVqe2hc_ 200.000_ 210.000.wav  not_covid\n","18     vMfc_I1Is_0_ 40.000_ 50.000.wav  not_covid\n","19      kWc9LXvShF4_ 0.000_ 10.000.wav  not_covid\n","20     PyXQVNaAbHc_ 30.000_ 40.000.wav  not_covid\n","21   j1JtHfcLG14_ 120.000_ 130.000.wav  not_covid\n","..                                 ...        ...\n","744  6lbkx_tf50g_ 220.000_ 230.000.wav  not_covid\n","745    KoUw2T0QKgY_ 70.000_ 80.000.wav  not_covid\n","746     X0kpF1at8lM_ 0.000_ 10.000.wav  not_covid\n","747    j7MwYU5VH1s_ 70.000_ 80.000.wav  not_covid\n","748    o-TJISpYLFc_ 50.000_ 60.000.wav  not_covid\n","\n","[732 rows x 2 columns]\n","covid_samples shuffled:                                               name  label\n","0   cough-shallow-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav  covid\n","1     cough-heavy-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav  covid\n","2     cough-heavy-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav  covid\n","3                      pos-0422-096-cough-m-31.wav  covid\n","4                      pos-0421-092-cough-m-53.wav  covid\n","5                      pos-0421-094-cough-m-51.wav  covid\n","6                      pos-0421-086-cough-m-65.wav  covid\n","7     cough-heavy-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav  covid\n","8     cough-heavy-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav  covid\n","9   cough-shallow-hte8VptUoGVFEqvHpbh5brgfcNP2.wav  covid\n","10                     pos-0421-093-cough-f-24.wav  covid\n","11                     pos-0421-087-cough-f-40.wav  covid\n","12                     pos-0421-084-cough-m-50.wav  covid\n","13  cough-shallow-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav  covid\n","14    cough-heavy-hte8VptUoGVFEqvHpbh5brgfcNP2.wav  covid\n","15  cough-shallow-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav  covid\n","16  cough-shallow-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav  covid\n","other_samples shuffled:                                  name      label\n","0   kkmpw7ERY-g_ 120.000_ 130.000.wav  not_covid\n","1   rTWz-KoNhyQ_ 250.000_ 260.000.wav  not_covid\n","2       -ej81N6Aqo4_ 0.000_ 8.000.wav  not_covid\n","3      oUUfZn8K_pg_ 0.000_ 10.000.wav  not_covid\n","4     AIVt3e5EVtc_ 70.000_ 80.000.wav  not_covid\n","5     ZoVIs72ONRA_ 70.000_ 80.000.wav  not_covid\n","6      pBFMlOQyXCg_ 0.000_ 10.000.wav  not_covid\n","7     _hptdlGvSV4_ 10.000_ 20.000.wav  not_covid\n","8      M3ZC3WCKkVM_ 0.000_ 10.000.wav  not_covid\n","9     kOheqjodDmw_ 50.000_ 60.000.wav  not_covid\n","10    -TbcaCBA0pI_ 50.000_ 60.000.wav  not_covid\n","11    tUUkucw-BOY_ 80.000_ 90.000.wav  not_covid\n","12    F_GGtPcT3Ew_ 30.000_ 40.000.wav  not_covid\n","13    F58fP5olXCA_ 20.000_ 30.000.wav  not_covid\n","14    gM4vFz4Zxtk_ 20.000_ 30.000.wav  not_covid\n","15    uTOrHGJQfqQ_ 30.000_ 40.000.wav  not_covid\n","16    HfBGqT5ss-o_ 60.000_ 70.000.wav  not_covid\n","17   VAQeoyf2C9s_ 90.000_ 100.000.wav  not_covid\n","18    lTGJX5f2scI_ 80.000_ 90.000.wav  not_covid\n","19  0QOYirw4e3I_ 270.000_ 280.000.wav  not_covid\n","20    rBdCfnD819k_ 10.000_ 20.000.wav  not_covid\n","21    AjbyESmmyts_ 70.000_ 80.000.wav  not_covid\n","22     3aFProJmJzY_ 0.000_ 10.000.wav  not_covid\n","23    AnFLES8rJfw_ 10.000_ 20.000.wav  not_covid\n","24    yMNs4gCJn1c_ 20.000_ 30.000.wav  not_covid\n","25     RWlyM4veldE_ 0.000_ 10.000.wav  not_covid\n","26  RwE9JAktTvU_ 580.000_ 590.000.wav  not_covid\n","27    UibT25x1yFs_ 10.000_ 20.000.wav  not_covid\n","28  7QkwmMU4w1M_ 220.000_ 230.000.wav  not_covid\n","29    qHxgEpRG1Vs_ 30.000_ 40.000.wav  not_covid\n","audio_names: ['cough-shallow-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav', 'cough-heavy-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav', 'cough-heavy-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav', 'pos-0422-096-cough-m-31.wav', 'pos-0421-092-cough-m-53.wav', 'pos-0421-094-cough-m-51.wav', 'pos-0421-086-cough-m-65.wav', 'cough-heavy-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav', 'cough-heavy-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav', 'cough-shallow-hte8VptUoGVFEqvHpbh5brgfcNP2.wav', 'pos-0421-093-cough-f-24.wav', 'pos-0421-087-cough-f-40.wav', 'pos-0421-084-cough-m-50.wav', 'cough-shallow-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav', 'cough-heavy-hte8VptUoGVFEqvHpbh5brgfcNP2.wav', 'cough-shallow-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav', 'cough-shallow-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav', 'kkmpw7ERY-g_ 120.000_ 130.000.wav', 'rTWz-KoNhyQ_ 250.000_ 260.000.wav', '-ej81N6Aqo4_ 0.000_ 8.000.wav', 'oUUfZn8K_pg_ 0.000_ 10.000.wav', 'AIVt3e5EVtc_ 70.000_ 80.000.wav', 'ZoVIs72ONRA_ 70.000_ 80.000.wav', 'pBFMlOQyXCg_ 0.000_ 10.000.wav', '_hptdlGvSV4_ 10.000_ 20.000.wav', 'M3ZC3WCKkVM_ 0.000_ 10.000.wav', 'kOheqjodDmw_ 50.000_ 60.000.wav', '-TbcaCBA0pI_ 50.000_ 60.000.wav', 'tUUkucw-BOY_ 80.000_ 90.000.wav', 'F_GGtPcT3Ew_ 30.000_ 40.000.wav', 'F58fP5olXCA_ 20.000_ 30.000.wav', 'gM4vFz4Zxtk_ 20.000_ 30.000.wav', 'uTOrHGJQfqQ_ 30.000_ 40.000.wav', 'HfBGqT5ss-o_ 60.000_ 70.000.wav', 'VAQeoyf2C9s_ 90.000_ 100.000.wav', 'lTGJX5f2scI_ 80.000_ 90.000.wav', '0QOYirw4e3I_ 270.000_ 280.000.wav', 'rBdCfnD819k_ 10.000_ 20.000.wav', 'AjbyESmmyts_ 70.000_ 80.000.wav', '3aFProJmJzY_ 0.000_ 10.000.wav', 'AnFLES8rJfw_ 10.000_ 20.000.wav', 'yMNs4gCJn1c_ 20.000_ 30.000.wav', 'RWlyM4veldE_ 0.000_ 10.000.wav', 'RwE9JAktTvU_ 580.000_ 590.000.wav', 'UibT25x1yFs_ 10.000_ 20.000.wav', '7QkwmMU4w1M_ 220.000_ 230.000.wav', 'qHxgEpRG1Vs_ 30.000_ 40.000.wav']\n","audio_labels: ['covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid', 'not_covid']\n","train_names: ['pos-0421-092-cough-m-53.wav', '-ej81N6Aqo4_ 0.000_ 8.000.wav', 'cough-heavy-hte8VptUoGVFEqvHpbh5brgfcNP2.wav', 'pos-0421-087-cough-f-40.wav', '-TbcaCBA0pI_ 50.000_ 60.000.wav', 'pBFMlOQyXCg_ 0.000_ 10.000.wav', 'cough-heavy-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav', 'cough-heavy-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav', '0QOYirw4e3I_ 270.000_ 280.000.wav', 'F58fP5olXCA_ 20.000_ 30.000.wav', 'ZoVIs72ONRA_ 70.000_ 80.000.wav', 'cough-shallow-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav', 'F_GGtPcT3Ew_ 30.000_ 40.000.wav', 'pos-0421-093-cough-f-24.wav', 'oUUfZn8K_pg_ 0.000_ 10.000.wav', 'AjbyESmmyts_ 70.000_ 80.000.wav', 'lTGJX5f2scI_ 80.000_ 90.000.wav', 'rBdCfnD819k_ 10.000_ 20.000.wav', '_hptdlGvSV4_ 10.000_ 20.000.wav', 'cough-heavy-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav', 'pos-0421-094-cough-m-51.wav', 'tUUkucw-BOY_ 80.000_ 90.000.wav', '7QkwmMU4w1M_ 220.000_ 230.000.wav', 'gM4vFz4Zxtk_ 20.000_ 30.000.wav', 'pos-0421-086-cough-m-65.wav', 'UibT25x1yFs_ 10.000_ 20.000.wav', 'cough-heavy-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav', 'yMNs4gCJn1c_ 20.000_ 30.000.wav', '3aFProJmJzY_ 0.000_ 10.000.wav', 'AnFLES8rJfw_ 10.000_ 20.000.wav', 'rTWz-KoNhyQ_ 250.000_ 260.000.wav', 'cough-shallow-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav'], num_train: 32\n","train_labels: [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n","test_names: ['pos-0422-096-cough-m-31.wav', 'qHxgEpRG1Vs_ 30.000_ 40.000.wav', 'VAQeoyf2C9s_ 90.000_ 100.000.wav', 'AIVt3e5EVtc_ 70.000_ 80.000.wav', 'cough-shallow-hte8VptUoGVFEqvHpbh5brgfcNP2.wav', 'HfBGqT5ss-o_ 60.000_ 70.000.wav', 'kkmpw7ERY-g_ 120.000_ 130.000.wav'], num_test: 7\n","test_labels: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n","Loaded audio file: dataset/audio/pos-0421-092-cough-m-53.wav, num samples, length: 425984 s\n","Loaded audio: pos-0421-092-cough-m-53.wav, Label: covid\n","MFCC initial shape: (12, 833)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/-ej81N6Aqo4_ 0.000_ 8.000.wav, num samples, length: 117772 s\n","Loaded audio: -ej81N6Aqo4_ 0.000_ 8.000.wav, Label: not_covid\n","MFCC initial shape: (12, 231)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-heavy-hte8VptUoGVFEqvHpbh5brgfcNP2.wav, num samples, length: 114688 s\n","Loaded audio: cough-heavy-hte8VptUoGVFEqvHpbh5brgfcNP2.wav, Label: covid\n","MFCC initial shape: (12, 225)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pos-0421-087-cough-f-40.wav, num samples, length: 281259 s\n","Loaded audio: pos-0421-087-cough-f-40.wav, Label: covid\n","MFCC initial shape: (12, 550)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/-TbcaCBA0pI_ 50.000_ 60.000.wav, num samples, length: 160000 s\n","Loaded audio: -TbcaCBA0pI_ 50.000_ 60.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pBFMlOQyXCg_ 0.000_ 10.000.wav, num samples, length: 160000 s\n","Loaded audio: pBFMlOQyXCg_ 0.000_ 10.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-heavy-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav, num samples, length: 59444 s\n","Loaded audio: cough-heavy-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav, Label: covid\n","MFCC initial shape: (12, 117)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-heavy-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav, num samples, length: 116054 s\n","Loaded audio: cough-heavy-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav, Label: covid\n","MFCC initial shape: (12, 227)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/0QOYirw4e3I_ 270.000_ 280.000.wav, num samples, length: 160000 s\n","Loaded audio: 0QOYirw4e3I_ 270.000_ 280.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/F58fP5olXCA_ 20.000_ 30.000.wav, num samples, length: 160000 s\n","Loaded audio: F58fP5olXCA_ 20.000_ 30.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/ZoVIs72ONRA_ 70.000_ 80.000.wav, num samples, length: 160000 s\n","Loaded audio: ZoVIs72ONRA_ 70.000_ 80.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-shallow-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav, num samples, length: 74304 s\n","Loaded audio: cough-shallow-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav, Label: covid\n","MFCC initial shape: (12, 146)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/F_GGtPcT3Ew_ 30.000_ 40.000.wav, num samples, length: 160000 s\n","Loaded audio: F_GGtPcT3Ew_ 30.000_ 40.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pos-0421-093-cough-f-24.wav, num samples, length: 243030 s\n","Loaded audio: pos-0421-093-cough-f-24.wav, Label: covid\n","MFCC initial shape: (12, 475)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/oUUfZn8K_pg_ 0.000_ 10.000.wav, num samples, length: 160000 s\n","Loaded audio: oUUfZn8K_pg_ 0.000_ 10.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/AjbyESmmyts_ 70.000_ 80.000.wav, num samples, length: 160000 s\n","Loaded audio: AjbyESmmyts_ 70.000_ 80.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/lTGJX5f2scI_ 80.000_ 90.000.wav, num samples, length: 160000 s\n","Loaded audio: lTGJX5f2scI_ 80.000_ 90.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/rBdCfnD819k_ 10.000_ 20.000.wav, num samples, length: 160000 s\n","Loaded audio: rBdCfnD819k_ 10.000_ 20.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/_hptdlGvSV4_ 10.000_ 20.000.wav, num samples, length: 160000 s\n","Loaded audio: _hptdlGvSV4_ 10.000_ 20.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-heavy-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav, num samples, length: 63902 s\n","Loaded audio: cough-heavy-6T43bddKoKfG7MwnJWvrPZSsyrc2.wav, Label: covid\n","MFCC initial shape: (12, 125)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pos-0421-094-cough-m-51.wav, num samples, length: 281259 s\n","Loaded audio: pos-0421-094-cough-m-51.wav, Label: covid\n","MFCC initial shape: (12, 550)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/tUUkucw-BOY_ 80.000_ 90.000.wav, num samples, length: 160000 s\n","Loaded audio: tUUkucw-BOY_ 80.000_ 90.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/7QkwmMU4w1M_ 220.000_ 230.000.wav, num samples, length: 160000 s\n","Loaded audio: 7QkwmMU4w1M_ 220.000_ 230.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/gM4vFz4Zxtk_ 20.000_ 30.000.wav, num samples, length: 160000 s\n","Loaded audio: gM4vFz4Zxtk_ 20.000_ 30.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pos-0421-086-cough-m-65.wav, num samples, length: 320854 s\n","Loaded audio: pos-0421-086-cough-m-65.wav, Label: covid\n","MFCC initial shape: (12, 627)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/UibT25x1yFs_ 10.000_ 20.000.wav, num samples, length: 160000 s\n","Loaded audio: UibT25x1yFs_ 10.000_ 20.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-heavy-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav, num samples, length: 4096 s\n","Loaded audio: cough-heavy-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav, Label: covid\n","MFCC initial shape: (12, 9)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/yMNs4gCJn1c_ 20.000_ 30.000.wav, num samples, length: 160000 s\n","Loaded audio: yMNs4gCJn1c_ 20.000_ 30.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/3aFProJmJzY_ 0.000_ 10.000.wav, num samples, length: 160000 s\n","Loaded audio: 3aFProJmJzY_ 0.000_ 10.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/AnFLES8rJfw_ 10.000_ 20.000.wav, num samples, length: 160000 s\n","Loaded audio: AnFLES8rJfw_ 10.000_ 20.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/rTWz-KoNhyQ_ 250.000_ 260.000.wav, num samples, length: 160000 s\n","Loaded audio: rTWz-KoNhyQ_ 250.000_ 260.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-shallow-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav, num samples, length: 162475 s\n","Loaded audio: cough-shallow-QjBZv868nydJzk0ZzwgKDHSG6Q82.wav, Label: covid\n","MFCC initial shape: (12, 318)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pos-0422-096-cough-m-31.wav, num samples, length: 318123 s\n","Loaded audio: pos-0422-096-cough-m-31.wav, Label: covid\n","MFCC initial shape: (12, 622)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/qHxgEpRG1Vs_ 30.000_ 40.000.wav, num samples, length: 160000 s\n","Loaded audio: qHxgEpRG1Vs_ 30.000_ 40.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/VAQeoyf2C9s_ 90.000_ 100.000.wav, num samples, length: 160000 s\n","Loaded audio: VAQeoyf2C9s_ 90.000_ 100.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/AIVt3e5EVtc_ 70.000_ 80.000.wav, num samples, length: 160000 s\n","Loaded audio: AIVt3e5EVtc_ 70.000_ 80.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-shallow-hte8VptUoGVFEqvHpbh5brgfcNP2.wav, num samples, length: 70998 s\n","Loaded audio: cough-shallow-hte8VptUoGVFEqvHpbh5brgfcNP2.wav, Label: covid\n","MFCC initial shape: (12, 139)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/HfBGqT5ss-o_ 60.000_ 70.000.wav, num samples, length: 160000 s\n","Loaded audio: HfBGqT5ss-o_ 60.000_ 70.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/kkmpw7ERY-g_ 120.000_ 130.000.wav, num samples, length: 160000 s\n","Loaded audio: kkmpw7ERY-g_ 120.000_ 130.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/uTOrHGJQfqQ_ 30.000_ 40.000.wav, num samples, length: 160000 s\n","Loaded audio: uTOrHGJQfqQ_ 30.000_ 40.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/pos-0421-084-cough-m-50.wav, num samples, length: 283990 s\n","Loaded audio: pos-0421-084-cough-m-50.wav, Label: covid\n","MFCC initial shape: (12, 555)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/kOheqjodDmw_ 50.000_ 60.000.wav, num samples, length: 160000 s\n","Loaded audio: kOheqjodDmw_ 50.000_ 60.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-shallow-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav, num samples, length: 75790 s\n","Loaded audio: cough-shallow-hNAGUEhL2Nh7V89at3yFEjQYo6c2.wav, Label: covid\n","MFCC initial shape: (12, 149)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/cough-shallow-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav, num samples, length: 8192 s\n","Loaded audio: cough-shallow-3CwioNQVDBQ6CttLyFVRJpMpVHk2.wav, Label: covid\n","MFCC initial shape: (12, 17)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/RWlyM4veldE_ 0.000_ 10.000.wav, num samples, length: 160000 s\n","Loaded audio: RWlyM4veldE_ 0.000_ 10.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/M3ZC3WCKkVM_ 0.000_ 10.000.wav, num samples, length: 160000 s\n","Loaded audio: M3ZC3WCKkVM_ 0.000_ 10.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","Loaded audio file: dataset/audio/RwE9JAktTvU_ 580.000_ 590.000.wav, num samples, length: 160000 s\n","Loaded audio: RwE9JAktTvU_ 580.000_ 590.000.wav, Label: not_covid\n","MFCC initial shape: (12, 313)\n","MFCC new shape: (1, 12, 32, 1)\n","X_train shape: (32, 12, 32, 1)\n","y_train: [1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n"," 1. 0. 1. 0. 0. 0. 0. 1.], shape: (32,)\n","X_val shape: (8, 12, 32, 1)\n","y_val: [0. 1. 0. 1. 1. 0. 0. 0.], shape: (8,)\n","X_test shape: (7, 12, 32, 1)\n","y_test: [1. 0. 0. 0. 1. 0. 0.], shape: (7,)\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv (Conv2D)                (None, 6, 16, 8)          648       \n","_________________________________________________________________\n","dropout (Dropout)            (None, 6, 16, 8)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 768)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 1538      \n","=================================================================\n","Total params: 2,186\n","Trainable params: 2,186\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","8/8 - 1s - loss: 8.3090 - accuracy: 0.5938 - val_loss: 4.9182 - val_accuracy: 0.1250\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.12500, saving model to saved_models/keras_model.h5\n","Epoch 2/50\n","8/8 - 0s - loss: 6.3867 - accuracy: 0.5312 - val_loss: 3.5744 - val_accuracy: 0.3750\n","\n","Epoch 00002: val_accuracy improved from 0.12500 to 0.37500, saving model to saved_models/keras_model.h5\n","Epoch 3/50\n","8/8 - 0s - loss: 3.1366 - accuracy: 0.6875 - val_loss: 3.8298 - val_accuracy: 0.7500\n","\n","Epoch 00003: val_accuracy improved from 0.37500 to 0.75000, saving model to saved_models/keras_model.h5\n","Epoch 4/50\n","8/8 - 0s - loss: 5.6711 - accuracy: 0.5938 - val_loss: 3.6302 - val_accuracy: 0.6250\n","\n","Epoch 00004: val_accuracy did not improve from 0.75000\n","Epoch 5/50\n","8/8 - 0s - loss: 2.2032 - accuracy: 0.5938 - val_loss: 3.4021 - val_accuracy: 0.6250\n","\n","Epoch 00005: val_accuracy did not improve from 0.75000\n","Epoch 6/50\n","8/8 - 0s - loss: 1.6927 - accuracy: 0.8750 - val_loss: 3.4801 - val_accuracy: 0.7500\n","\n","Epoch 00006: val_accuracy did not improve from 0.75000\n","Epoch 7/50\n","8/8 - 0s - loss: 1.4422 - accuracy: 0.7500 - val_loss: 3.1763 - val_accuracy: 0.6250\n","\n","Epoch 00007: val_accuracy did not improve from 0.75000\n","Epoch 8/50\n","8/8 - 0s - loss: 0.6886 - accuracy: 0.8438 - val_loss: 2.8381 - val_accuracy: 0.6250\n","\n","Epoch 00008: val_accuracy did not improve from 0.75000\n","Epoch 9/50\n","8/8 - 0s - loss: 0.4957 - accuracy: 0.8125 - val_loss: 3.0443 - val_accuracy: 0.6250\n","\n","Epoch 00009: val_accuracy did not improve from 0.75000\n","Epoch 10/50\n","8/8 - 0s - loss: 0.8915 - accuracy: 0.8125 - val_loss: 3.0034 - val_accuracy: 0.6250\n","\n","Epoch 00010: val_accuracy did not improve from 0.75000\n","Epoch 11/50\n","8/8 - 0s - loss: 0.1797 - accuracy: 0.8750 - val_loss: 3.0647 - val_accuracy: 0.6250\n","\n","Epoch 00011: val_accuracy did not improve from 0.75000\n","Epoch 12/50\n","8/8 - 0s - loss: 0.1061 - accuracy: 0.9688 - val_loss: 3.0183 - val_accuracy: 0.6250\n","\n","Epoch 00012: val_accuracy did not improve from 0.75000\n","Epoch 13/50\n","8/8 - 0s - loss: 0.4135 - accuracy: 0.8750 - val_loss: 2.5499 - val_accuracy: 0.6250\n","\n","Epoch 00013: val_accuracy did not improve from 0.75000\n","Epoch 14/50\n","8/8 - 0s - loss: 0.3997 - accuracy: 0.9375 - val_loss: 2.4064 - val_accuracy: 0.6250\n","\n","Epoch 00014: val_accuracy did not improve from 0.75000\n","Epoch 15/50\n","8/8 - 0s - loss: 0.1396 - accuracy: 0.9688 - val_loss: 2.3998 - val_accuracy: 0.6250\n","\n","Epoch 00015: val_accuracy did not improve from 0.75000\n","Epoch 16/50\n","8/8 - 0s - loss: 0.1351 - accuracy: 0.9688 - val_loss: 2.3507 - val_accuracy: 0.6250\n","\n","Epoch 00016: val_accuracy did not improve from 0.75000\n","Epoch 17/50\n","8/8 - 0s - loss: 0.1176 - accuracy: 0.9688 - val_loss: 2.3755 - val_accuracy: 0.6250\n","\n","Epoch 00017: val_accuracy did not improve from 0.75000\n","Epoch 18/50\n","8/8 - 0s - loss: 0.0767 - accuracy: 0.9688 - val_loss: 2.5861 - val_accuracy: 0.6250\n","\n","Epoch 00018: val_accuracy did not improve from 0.75000\n","Epoch 19/50\n","8/8 - 0s - loss: 0.0724 - accuracy: 0.9688 - val_loss: 2.8770 - val_accuracy: 0.6250\n","\n","Epoch 00019: val_accuracy did not improve from 0.75000\n","Epoch 20/50\n","8/8 - 0s - loss: 0.0979 - accuracy: 0.9688 - val_loss: 3.1472 - val_accuracy: 0.6250\n","\n","Epoch 00020: val_accuracy did not improve from 0.75000\n","Epoch 21/50\n","8/8 - 0s - loss: 0.0933 - accuracy: 0.9688 - val_loss: 3.2656 - val_accuracy: 0.6250\n","\n","Epoch 00021: val_accuracy did not improve from 0.75000\n","Epoch 22/50\n","8/8 - 0s - loss: 0.2692 - accuracy: 0.8750 - val_loss: 2.9621 - val_accuracy: 0.6250\n","\n","Epoch 00022: val_accuracy did not improve from 0.75000\n","Epoch 23/50\n","8/8 - 0s - loss: 0.0300 - accuracy: 1.0000 - val_loss: 2.8772 - val_accuracy: 0.6250\n","\n","Epoch 00023: val_accuracy did not improve from 0.75000\n","Epoch 24/50\n","8/8 - 0s - loss: 0.5931 - accuracy: 0.9062 - val_loss: 3.0308 - val_accuracy: 0.6250\n","\n","Epoch 00024: val_accuracy did not improve from 0.75000\n","Epoch 25/50\n","8/8 - 0s - loss: 0.1477 - accuracy: 0.9375 - val_loss: 2.8747 - val_accuracy: 0.6250\n","\n","Epoch 00025: val_accuracy did not improve from 0.75000\n","Epoch 26/50\n","8/8 - 0s - loss: 0.1510 - accuracy: 0.9375 - val_loss: 2.5331 - val_accuracy: 0.6250\n","\n","Epoch 00026: val_accuracy did not improve from 0.75000\n","Epoch 27/50\n","8/8 - 0s - loss: 0.3578 - accuracy: 0.9062 - val_loss: 2.2802 - val_accuracy: 0.6250\n","\n","Epoch 00027: val_accuracy did not improve from 0.75000\n","Epoch 28/50\n","8/8 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.1135 - val_accuracy: 0.6250\n","\n","Epoch 00028: val_accuracy did not improve from 0.75000\n","Epoch 29/50\n","8/8 - 0s - loss: 0.3747 - accuracy: 0.9062 - val_loss: 2.0368 - val_accuracy: 0.6250\n","\n","Epoch 00029: val_accuracy did not improve from 0.75000\n","Epoch 30/50\n","8/8 - 0s - loss: 0.1759 - accuracy: 0.9688 - val_loss: 2.0305 - val_accuracy: 0.6250\n","\n","Epoch 00030: val_accuracy did not improve from 0.75000\n","Epoch 31/50\n","8/8 - 0s - loss: 0.0741 - accuracy: 0.9688 - val_loss: 2.3138 - val_accuracy: 0.7500\n","\n","Epoch 00031: val_accuracy did not improve from 0.75000\n","Epoch 32/50\n","8/8 - 0s - loss: 0.2073 - accuracy: 0.9375 - val_loss: 2.2903 - val_accuracy: 0.7500\n","\n","Epoch 00032: val_accuracy did not improve from 0.75000\n","Epoch 33/50\n","8/8 - 0s - loss: 0.1252 - accuracy: 0.9688 - val_loss: 2.1380 - val_accuracy: 0.6250\n","\n","Epoch 00033: val_accuracy did not improve from 0.75000\n","Epoch 34/50\n","8/8 - 0s - loss: 0.1108 - accuracy: 0.9375 - val_loss: 1.9144 - val_accuracy: 0.6250\n","\n","Epoch 00034: val_accuracy did not improve from 0.75000\n","Epoch 35/50\n","8/8 - 0s - loss: 0.0926 - accuracy: 0.9688 - val_loss: 1.7578 - val_accuracy: 0.6250\n","\n","Epoch 00035: val_accuracy did not improve from 0.75000\n","Epoch 36/50\n","8/8 - 0s - loss: 0.1389 - accuracy: 0.9375 - val_loss: 2.1289 - val_accuracy: 0.6250\n","\n","Epoch 00036: val_accuracy did not improve from 0.75000\n","Epoch 37/50\n","8/8 - 0s - loss: 0.1760 - accuracy: 0.9688 - val_loss: 2.7251 - val_accuracy: 0.6250\n","\n","Epoch 00037: val_accuracy did not improve from 0.75000\n","Epoch 38/50\n","8/8 - 0s - loss: 0.2440 - accuracy: 0.9375 - val_loss: 2.6227 - val_accuracy: 0.6250\n","\n","Epoch 00038: val_accuracy did not improve from 0.75000\n","Epoch 39/50\n","8/8 - 0s - loss: 0.0347 - accuracy: 0.9688 - val_loss: 1.9013 - val_accuracy: 0.6250\n","\n","Epoch 00039: val_accuracy did not improve from 0.75000\n","Epoch 40/50\n","8/8 - 0s - loss: 0.0686 - accuracy: 0.9688 - val_loss: 1.8179 - val_accuracy: 0.6250\n","\n","Epoch 00040: val_accuracy did not improve from 0.75000\n","Epoch 41/50\n","8/8 - 0s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 1.8811 - val_accuracy: 0.6250\n","\n","Epoch 00041: val_accuracy did not improve from 0.75000\n","Epoch 42/50\n","8/8 - 0s - loss: 0.0746 - accuracy: 0.9688 - val_loss: 1.8737 - val_accuracy: 0.6250\n","\n","Epoch 00042: val_accuracy did not improve from 0.75000\n","Epoch 43/50\n","8/8 - 0s - loss: 0.0288 - accuracy: 1.0000 - val_loss: 2.0664 - val_accuracy: 0.6250\n","\n","Epoch 00043: val_accuracy did not improve from 0.75000\n","Epoch 44/50\n","8/8 - 0s - loss: 0.1263 - accuracy: 0.9688 - val_loss: 2.1385 - val_accuracy: 0.6250\n","\n","Epoch 00044: val_accuracy did not improve from 0.75000\n","Epoch 45/50\n","8/8 - 0s - loss: 0.1992 - accuracy: 0.9688 - val_loss: 1.9436 - val_accuracy: 0.6250\n","\n","Epoch 00045: val_accuracy did not improve from 0.75000\n","Epoch 46/50\n","8/8 - 0s - loss: 0.0886 - accuracy: 0.9375 - val_loss: 1.8733 - val_accuracy: 0.6250\n","\n","Epoch 00046: val_accuracy did not improve from 0.75000\n","Epoch 47/50\n","8/8 - 0s - loss: 0.0256 - accuracy: 1.0000 - val_loss: 1.9911 - val_accuracy: 0.6250\n","\n","Epoch 00047: val_accuracy did not improve from 0.75000\n","Epoch 48/50\n","8/8 - 0s - loss: 0.1143 - accuracy: 0.9375 - val_loss: 2.1714 - val_accuracy: 0.6250\n","\n","Epoch 00048: val_accuracy did not improve from 0.75000\n","Epoch 49/50\n","8/8 - 0s - loss: 0.1097 - accuracy: 0.9375 - val_loss: 2.4223 - val_accuracy: 0.6250\n","\n","Epoch 00049: val_accuracy did not improve from 0.75000\n","Epoch 50/50\n","8/8 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.4842 - val_accuracy: 0.6250\n","\n","Epoch 00050: val_accuracy did not improve from 0.75000\n","Training completed in 3.30 s\n","Training accuracy: 100.000\n","Testing accuracy: 85.714\n","WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f503352c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","Target labels (test set): [1. 0. 0. 0. 1. 0. 0.]\n","Predicted labels (test set): [1 0 0 0 0 0 0]\n","              precision    recall  f1-score   support\n","\n","   not_covid       0.83      1.00      0.91         5\n","       covid       1.00      0.50      0.67         2\n","\n","    accuracy                           0.86         7\n","   macro avg       0.92      0.75      0.79         7\n","weighted avg       0.88      0.86      0.84         7\n","\n"],"name":"stdout"}]}]}